<?xml version="1.0" encoding="UTF-8"?>
<chapter id="index"><?dbhtml dir="elasticsearch" ?>
	<title>Elasticsearch</title>
	<para>http://www.elasticsearch.org/</para>
	<section id="install">
		<title>安装 Elasticsearch</title>
		<section id="single">
			<title>单机模式 (适用于开发环境)</title>
			<para>使用 Netkiller OSCM 一键安装 Elasticsearch 5.6.0</para>
			<screen>
# Java
curl -s https://raw.githubusercontent.com/oscm/shell/master/lang/java/openjdk/java-1.8.0-openjdk.sh | bash

# Install
curl -s https://raw.githubusercontent.com/oscm/shell/master/search/elasticsearch/elasticsearch-5.x.sh | bash

# Bind 0.0.0.0
curl -s https://raw.githubusercontent.com/oscm/shell/master/search/elasticsearch/network.bind_host.sh | bash

# Auto create index
curl -s https://raw.githubusercontent.com/oscm/shell/master/search/elasticsearch/action.auto_create_index.sh | bash

# elasticsearch-analysis-ik

curl -s https://raw.githubusercontent.com/oscm/shell/master/search/elasticsearch/5.5/elasticsearch-analysis-ik-5.6.0.sh | bash
			</screen>
			<para>通常 elasticsearch-analysis-ik 的版本会比 elasticsearch 慢一个版本，所以请使用下面命令查看版本是否一致，如果不一致可以修改 plugin-descriptor.properties 配置文件，使其一致。</para>
			<screen>
root@netkiller /usr/share/elasticsearch/plugins/ik % grep ^version plugin-descriptor.properties
version=5.5.1
			</screen>
			<para>启动后使用 jps 命令检查进城是否工作正常</para>
			<screen>
root@netkiller /var/log/elasticsearch % jps | grep Elasticsearch
9706 Elasticsearch

root@netkiller /var/log/elasticsearch % ss -lnt | grep 9200
LISTEN     0      128    127.0.0.1:9200                     *:*
			</screen>
		</section>
		<section id="cluster">
			<title>Elasticsearch Cluster</title>
			<para>集群模式需要两个以上的节点，通常是一个 master 节点，多个 data 节点</para>
			<para>首先在所有节点上安装 elasticsearch，然后配置各节点的配置文件，对于 5.5.1 不需要配置决定哪些节点属于 master 节点 或者 data 节点。</para>
			<screen>
curl -s https://raw.githubusercontent.com/oscm/shell/master/search/elasticsearch/elasticsearch-5.x.sh | bash			
			</screen>
			<para>配置文件</para>
			<screen>
cluster.name: elasticsearch-cluster # 配置集群名称,所有服务器服务器保持一致

node.name: node-1 # 每个节点唯一标识，每个节点只需改动这里，一次递增 node-1, node-2, node-3 ...

network.host: 0.0.0.0

discovery.zen.ping.unicast.hosts: ["172.16.0.20", "172.16.0.21","172.16.0.22"]  # 所有节点的IP 地址写在这里

discovery.zen.minimum_master_nodes: 3 # 可以作为master的节点总数，有多少个节点就写多少

http.cors.enabled: true
http.cors.allow-origin: "*"
			</screen>
			<para>查看节点状态，使用curl工具: curl 'http://localhost:9200/_nodes/process?pretty'</para>
			<screen>
root@netkiller /var/log/elasticsearch % curl 'http://localhost:9200/_nodes/process?pretty'
{
  "_nodes" : {
    "total" : 2,
    "successful" : 2,
    "failed" : 0
  },
  "cluster_name" : "my-application",
  "nodes" : {
    "-lnKCmBXRpiwExLns0jc9g" : {
      "name" : "node-1",
      "transport_address" : "10.104.3.2:9300",
      "host" : "10.104.3.2",
      "ip" : "10.104.3.2",
      "version" : "5.5.1",
      "build_hash" : "19c13d0",
      "roles" : [
        "master",
        "data",
        "ingest"
      ],
      "process" : {
        "refresh_interval_in_millis" : 1000,
        "id" : 23669,
        "mlockall" : false
      }
    },
    "WVsgYi2HT8GWnZU1kUwFwA" : {
      "name" : "node-2",
      "transport_address" : "10.186.7.221:9300",
      "host" : "10.186.7.221",
      "ip" : "10.186.7.221",
      "version" : "5.5.1",
      "build_hash" : "19c13d0",
      "roles" : [
        "master",
        "data",
        "ingest"
      ],
      "process" : {
        "refresh_interval_in_millis" : 1000,
        "id" : 12641,
        "mlockall" : false
      }
    }
  }
}
			</screen>
			<para>启动节点后回生成 cluster.name 为文件名的日志文件。</para>
			<para>谁先启动谁讲成为master</para>
			<screen>
[2017-08-11T17:42:46,018][INFO ][o.e.c.s.ClusterService   ] [node-1] new_master {node-1}{-lnKCmBXRpiwExLns0jc9g}{rZcJDIynSzq2Td3yP2kN5A}{10.104.3.2}{10.104.3.2:9300}, added {{node-2}{WVsgYi2HT8GWnZU1kUwFwA}{X13ShUpAQa2zA1Mgcsm3bQ}{10.186.7.221}{10.186.7.221:9300},}, reason: zen-disco-elected-as-master ([1] nodes joined)[{node-2}{WVsgYi2HT8GWnZU1kUwFwA}{X13ShUpAQa2zA1Mgcsm3bQ}{10.186.7.221}{10.186.7.221:9300}]			
			</screen>
			<para>如果master出现故障，其他节点会接管</para>
			<screen>
[2017-08-11T17:44:52,797][INFO ][o.e.c.s.ClusterService   ] [node-2] master {new {node-2}{WVsgYi2HT8GWnZU1kUwFwA}{vl8kQx8sQdGVVohrNQnZOQ}{10.186.7.221}{10.186.7.221:9300}}, removed {{node-1}{-lnKCmBXRpiwExLns0jc9g}{rZcJDIynSzq2Td3yP2kN5A}{10.104.3.2}{10.104.3.2:9300},}, added {{node-1}{-lnKCmBXRpiwExLns0jc9g}{odnoG9kpQpeX1ltx5KYTSw}{10.104.3.2}{10.104.3.2:9300},}, reason: zen-disco-elected-as-master ([1] nodes joined)[{node-1}{-lnKCmBXRpiwExLns0jc9g}{odnoG9kpQpeX1ltx5KYTSw}{10.104.3.2}{10.104.3.2:9300}]
[2017-08-11T17:44:53,184][INFO ][o.e.c.r.DelayedAllocationService] [node-2] scheduling reroute for delayed shards in [59.5s] (11 delayed shards)
[2017-08-11T17:44:53,929][INFO ][o.e.c.r.a.AllocationService] [node-2] Cluster health status changed from [RED] to [YELLOW] (reason: [shards started [[information][0]] ...]).		
			</screen>
			<para>master 节点恢复上线会提示</para>
			<screen>
[2017-08-11T17:44:52,855][INFO ][o.e.c.s.ClusterService   ] [node-1] detected_master {node-2}{WVsgYi2HT8GWnZU1kUwFwA}{vl8kQx8sQdGVVohrNQnZOQ}{10.186.7.221}{10.186.7.221:9300}, added {{node-2}{WVsgYi2HT8GWnZU1kUwFwA}{vl8kQx8sQdGVVohrNQnZOQ}{10.186.7.221}{10.186.7.221:9300},}, reason: zen-disco-receive(from master [master {node-2}{WVsgYi2HT8GWnZU1kUwFwA}{vl8kQx8sQdGVVohrNQnZOQ}{10.186.7.221}{10.186.7.221:9300} committed version [44]])
			</screen>
		</section>
		<section id="loadbalance">
			<title>负载均衡配置</title>
			<para>首先安装 nginx, 这里使用 Netkiller OSCM 一键安装脚本完成。</para>
			<screen>
# curl -s https://raw.githubusercontent.com/oscm/shell/master/web/nginx/stable/nginx.sh | bash
			</screen>
			<para>因为 elasticsearch 没有用户认证机制我们通常在内网访问他。如果对外提供服务需要增加用户认证。</para>
			<para></para>
			<screen>
			<![CDATA[
# printf "neo:$(openssl passwd -crypt s3cr3t)n" > /etc/nginx/passwords 			
			]]>
			</screen>
			<para>创建 nginx 配置文件 /etc/nginx/conf.d/elasticsearch.conf</para>
			<screen>
upstream elasticsearch {
	server 172.16.0.10:9200;
	server 172.16.0.20:9200;
	server 172.16.0.30:9200;

	keepalive 15;
}

server {
	listen 9200;
	server_name so.netkiller.cn;
	
	charset utf-8;
    access_log /var/log/nginx/so.netkiller.cn.access.log;
    error_log /var/log/nginx/so.netkiller.cn.error.log;
	
	auth_basic "Protected Elasticsearch";
	auth_basic_user_file passwords;

	location ~* ^(/_cluster|/_nodes) {
		return 403;
		break;
	}
    location ~* _(open|close) {
            return 403;
            break;
    }
	location / {
    
		if ($request_filename ~ _shutdown) {
		    return 403;
		    break;
		}

        if ($request_method !~ ^(GET|HEAD|POST)$) {
			return 403;
		}

		proxy_pass http://elasticsearch;
		proxy_http_version 1.1;
		proxy_set_header Connection "Keep-Alive";
		proxy_set_header Proxy-Connection "Keep-Alive";
	}

}
			</screen>
			<para>反复使用下面方法请求，最终你会发现 total_opened 会达到你的nginx 配置数量</para>
			<screen>
$ curl 'http://test:test@localhost:9200/_nodes/stats/http?pretty' | grep total_opened
# "total_opened" : 15			
			</screen>
			<para>上面的例子适用于绝大多数场景。</para>
			<example>
				<title>Elasticsearch master / slave</title>
				<screen>
				<![CDATA[
upstream elasticsearch {
	server 172.16.0.10:9200;
	server 172.16.0.20:9200 backup;

	keepalive 15;
}

server {
	listen 9200;
	server_name so.netkiller.cn;
	
	auth_basic "Protected Elasticsearch";
	auth_basic_user_file passwords;

	location ~* ^(/_cluster|/_nodes) {
		return 403;
		break;
	} 

	location / {
    
		if ($request_filename ~ _shutdown) {
		    return 403;
		    break;
		}
		if ($request_method !~ "HEAD") {
          return 403;
          break;
        }
        if ($request_method ~ "DELETE") {
          return 403;
          break;
        }

		proxy_pass http://elasticsearch;
		proxy_http_version 1.1;
		proxy_set_header Connection "Keep-Alive";
		proxy_set_header Proxy-Connection "Keep-Alive";
	}

}
				]]>
				</screen>
			</example>
			<para>通过 limit_except 可以控制访问权限，例如删除操作。</para>
			<screen>
			<![CDATA[
limit_except PUT {
	allow 192.168.1.1;
	deny all;
}
limit_except DELETE {
	allow 192.168.1.1;
	deny all;
}
			]]>
			</screen>
		</section>
		<section id="version">
			<title>安装指定版本的 Elasticsearch</title>
			<para>使用 yum 安装默认为最新版本，我们常常会遇到一个问题 elasticsearch-analysis-ik 的版本晚于 Elasticsearch。如果使用 yum 安装 Elasticsearch 可能 elasticsearch-analysis-ik 插件不支持这个版本，有些版本的 elasticsearch-analysis-ik 可以修改插件配置文件中的版本号，使其与elasticsearch版本相同，可以欺骗 elasticsearch 跳过版本不一致异常。</para>
			<para>最佳的解决方案是去 <ulink url="https://github.com/medcl/elasticsearch-analysis-ik" >elasticsearch-analysis-ik github</ulink> 找到兼容的版本，安装我们安装 elasticsearch-analysis-ik 的版本需求来指定安装 elasticsearch</para>
			<para></para>
			<screen>
Versions

IK version	ES version
master	5.x -> master
5.6.0	5.6.0
5.5.3	5.5.3
5.4.3	5.4.3
5.3.3	5.3.3
5.2.2	5.2.2
5.1.2	5.1.2
1.10.1	2.4.1
1.9.5	2.3.5
1.8.1	2.2.1
1.7.0	2.1.1
1.5.0	2.0.0
1.2.6	1.0.0
1.2.5	0.90.x
1.1.3	0.20.x
1.0.0	0.16.2 -> 0.19.0			
			</screen>
			<para>最新版是 elasticsearch 5.6.1 但分词插件 elasticsearch-analysis-ik 仅能支持到 elasticsearch 版本是 5.6.0 </para>
			<screen>
root@netkiller /var/log % yum --showduplicates list elasticsearch | expand | tail
Repository epel is listed more than once in the configuration  
elasticsearch.noarch                 5.5.3-1                  elasticsearch-5.x     
elasticsearch.noarch                 5.6.0-1                  elasticsearch-5.x   
elasticsearch.noarch                 5.6.1-1                  elasticsearch-5.x 
			</screen>
			<para>安装 5.6.0</para>
			<screen>
# yum install elasticsearch-5.6.0-1

Loaded plugins: fastestmirror, langpacks
Repository epel is listed more than once in the configuration
Loading mirror speeds from cached hostfile
Resolving Dependencies
--> Running transaction check
---> Package elasticsearch.noarch 0:5.6.0-1 will be installed
--> Finished Dependency Resolution

Dependencies Resolved

==========================================================================================================================================================================================================
 Package                                            Arch                                        Version                                      Repository                                              Size
==========================================================================================================================================================================================================
Installing:
 elasticsearch                                      noarch                                      5.6.0-1                                      elasticsearch-5.x                                       32 M

Transaction Summary
==========================================================================================================================================================================================================
Install  1 Package

Total download size: 32 M
Installed size: 36 M
Is this ok [y/d/N]: y
			</screen>

		</section>
		<section id="plugin">
			<title>Plugin</title>
			<para>Elasticsearch 提供了插件管理命令 elasticsearch-plugin</para>
			<screen>
root@netkiller ~ % /usr/share/elasticsearch/bin/elasticsearch-plugin -h
A tool for managing installed elasticsearch plugins

Commands
--------
list - Lists installed elasticsearch plugins
install - Install a plugin
remove - removes a plugin from Elasticsearch

Non-option arguments:
command              

Option         Description        
------         -----------        
-h, --help     show help          
-s, --silent   show minimal output
-v, --verbose  show verbose output			
			</screen>
			<section id="elasticsearch-analysis-ik">
				<title>elasticsearch-analysis-ik</title>
				<para>安装插件</para>
				<screen>
root@netkiller ~ % /usr/share/elasticsearch/bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v5.5.1/elasticsearch-analysis-ik-5.5.1.zip
-> Downloading https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v5.5.1/elasticsearch-analysis-ik-5.5.1.zip
[=================================================] 100%   
-> Installed analysis-ik
				</screen>
				<screen>
curl -XPOST http://localhost:9200/index/fulltext/_mapping -d'
{
        "properties": {
            "content": {
                "type": "text",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word"
            }
        }
    
}'			
				</screen>
			</section>
			<section id="elasticsearch-analysis-pinyin">
				<title>elasticsearch-analysis-pinyin</title>
				<para>https://github.com/medcl/elasticsearch-analysis-pinyin</para>
			</section>
		</section>
	</section>
	
	
	<section id="document">
		<title>文档API</title>
		<section id="quickstart">
			<title>快速上手</title>
			<para>文档通过 _index、_type、_id 元数据(metadata)，确定 URL 唯一</para>
			<screen>
			<![CDATA[
GET /<_index>/<_type>/<_id>		
			]]>		
			</screen>
			<screen>
# curl -XPUT 'http://localhost:9200/website/profile/1' -d '{
	"name" : "neo",
	"nickname" : "netkiller",
	"age" : "35",
	"message" : "Helloworld !!!"
}'

# curl -XGET 'http://localhost:9200/website/profile/1?pretty'
{
  "_index" : "website",
  "_type" : "profile",
  "_id" : "1",
  "_version" : 1,
  "found" : true,
  "_source" : {
    "name" : "neo",
    "nickname" : "netkiller",
    "age" : "35",
    "message" : "Helloworld !!!"
  }
}

# curl -XPUT 'http://localhost:9200/website/blog/1?pretty' -d '{
>   "title": "My first blog entry",
>   "text":  "Just trying this out...",
>   "date":  "2014/01/01"
> }'
{
  "_index" : "website",
  "_type" : "blog",
  "_id" : "1",
  "_version" : 1,
  "_shards" : {
    "total" : 2,
    "successful" : 1,
    "failed" : 0
  },
  "created" : true
}
			</screen>
			<para>后面会详细讲解 PUT与GET的使用方法以及相关参数</para>
		</section>
		<section id="put">
			<title>写入 PUT/POST</title>
			<para>通过 PUT 写入数据</para>
			<screen>
[root@localhost ~]# curl -XPUT 'http://localhost:9200/twitter/tweet/1' -d '{
>     "user" : "kimchy",
>     "post_date" : "2009-11-15T14:12:12",
>     "message" : "trying out Elasticsearch"
> }'
{"_index":"twitter","_type":"tweet","_id":"1","_version":1,"_shards":{"total":2,"successful":1,"failed":0},"created":true}			
			</screen>
			
			<para>使用 UUID 替代 _id, 注意使用UUID 必须使用 POST方式提交，不能使用 PUT。</para>
			<screen>
curl -XPOST 'http://localhost:9200/website/news/?pretty' -d '{
  "title": "My first news entry",
  "text":  "Just trying this out..."
}'
{
  "_index" : "website",
  "_type" : "news1",
  "_id" : "AVY0RJrvJRTrBLpmYzBH",
  "_version" : 1,
  "_shards" : {
    "total" : 2,
    "successful" : 1,
    "failed" : 0
  },
  "created" : true
}

curl -XGET 'http://localhost:9200/website/news/AVY0RJrvJRTrBLpmYzBH?pretty'
			</screen>
			<para>提交后会输出 "_id" : "AVY0RJrvJRTrBLpmYzBH"，查询时将此放到放到URL中即可。</para>
			
		</section>
		
		<section id="get">
			<title>获取 GET</title>
			<para>通过 GET 读取数据</para>
			<screen>
[root@localhost ~]# curl -XGET 'http://localhost:9200/twitter/tweet/1'
{"_index":"twitter","_type":"tweet","_id":"1","_version":1,"found":true,"_source":{
    "user" : "kimchy",
    "post_date" : "2009-11-15T14:12:12",
    "message" : "trying out Elasticsearch"
}}
			</screen>
			<section>
				<title>_source</title>
				<para>只返回 _source 数据，去掉元数据</para>
				<screen>
# curl -XGET 'http://localhost:9200/website/news1/AVY0Q4SqdtH0Up0t-WB2/_source?pretty'
{
  "title" : "My first news entry",
  "text" : "Just trying this out..."
}
				</screen>
				<para>选择字段 _source=title，超过一个字段使用逗号分隔_source=title,text。</para>
				<screen>
				<![CDATA[
# curl -XGET 'http://localhost:9200/website/news1/AVY0Q4SqdtH0Up0t-WB2?_source=title&pretty'
{
  "_index" : "website",
  "_type" : "news1",
  "_id" : "AVY0Q4SqdtH0Up0t-WB2",
  "_version" : 1,
  "found" : true,
  "_source" : {
    "title" : "My first news entry"
  }
}

# curl -XGET 'http://localhost:9200/website/news1/AVY0Q4SqdtH0Up0t-WB2?_source=title,text&pretty'
{
  "_index" : "website",
  "_type" : "news1",
  "_id" : "AVY0Q4SqdtH0Up0t-WB2",
  "_version" : 1,
  "found" : true,
  "_source" : {
    "text" : "Just trying this out...",
    "title" : "My first news entry"
  }
}

				]]>
				</screen>
			</section>
		</section>
		<section id="head">
			<title>检查记录是否存在</title>
			<screen>
[root@localhost elasticsearch]# curl -i -XHEAD http://localhost:9200/website/blog/1
HTTP/1.1 200 OK
Content-Type: text/plain; charset=UTF-8
Content-Length: 0

[root@localhost elasticsearch]# curl -i -XHEAD http://localhost:9200/website/blog/100
HTTP/1.1 404 Not Found
Content-Type: text/plain; charset=UTF-8
Content-Length: 0
			</screen>
			<para>HTTP/1.1 200 OK 表示已经找到你要的数据</para>
			<para>HTTP/1.1 404 Not Found 表示数据不存在</para>
		</section>
		<section id="delete">
			<title>删除 Delete</title>
			<para>删除 _index</para>
			<screen>
curl -XDELETE http://localhost:9200/information/?pretty
			</screen>
			<para>删除 _mapping</para>
			<screen>
curl -XDELETE http://localhost:9200/information/news/_mapping?pretty			
			</screen>
			<para>删除对象</para>
			<screen>
curl -XDELETE http://localhost:9200/information/news/1?pretty			
			</screen>
		</section>
		<section id="param">
			<title>参数</title>
			<section>
				<title>pretty 格式化 json</title>
				<screen>
# curl -XGET 'http://localhost:9200/twitter/tweet/1?pretty'
{
  "_index" : "twitter",
  "_type" : "tweet",
  "_id" : "1",
  "_version" : 1,
  "found" : true,
  "_source" : {
    "user" : "kimchy",
    "post_date" : "2009-11-15T14:12:12",
    "message" : "trying out Elasticsearch"
  }
}			
				</screen>
			</section>
		</section>
	</section>
	<section id="search">
		<title>搜索</title>
		<para>搜索所有内容</para>
		<screen>
# curl -XGET 'http://localhost:9200/_search?pretty'	
# curl -XGET 'http://localhost:9200/_all/_search?pretty'			
		</screen>
		<para>指定 _index 搜索</para>
		<screen>
# curl -XGET 'http://localhost:9200/website/_search?pretty'
# curl -XGET 'http://localhost:9200/website/news/_search?pretty'
		</screen>
		<para>指定 _type 搜索</para>
		<screen>
# curl -XGET 'http://localhost:9200/website,twitter/_search?pretty'
# curl -XGET 'http://localhost:9200/website/news,blog/_search?pretty'
# curl -XGET 'http://localhost:9200/website,twitter/news,blog/_search?pretty'
		</screen>
		<para>所有 _index 包含指定 _type 搜索</para>
		<screen>
# curl -XGET 'http://localhost:9200/_all/news,blog/_search?pretty'
		</screen>
		
		
		<section id="query">
			<title>URL 搜索</title>
			<para>字符串搜索</para>
			<screen>
			<![CDATA[
# curl -XGET 'http://localhost:9200/_all/_search?q=neo&pretty'
			]]>			
			</screen>
			<para>同时满足两个条件</para>
			<screen>
+name:neo +age:30
			</screen>
			<para>查找name为mary 或者 john的数据</para>
			<screen>
+name:(mary john)
			</screen>
			<para>查询姓名是neo或者jam并且年龄小于30岁同时1980-09-10之后出生的</para>
			<screen>
			<![CDATA[
+name:(neo jam) +age:<30 +date:>1980-09-10
			]]>
			</screen>
		</section><section id="search.page">
			<title>分页</title>
			<para>该功能与SQL的LIMIT关键字结果一样，Elasticsearch接受size和from两个参数参数：</para>
			<para>size: 返回结果集数量，默认10，用法与SQL中的 Limit相同</para>
			<para>from: 偏移量，默认0，用法与 SQL中的 Offset相同</para>
			<para>如果你想每页显示10个结果，那么请求如下：</para>
			<screen>
			<![CDATA[
第一页 GET /_search?size=10
第二页 GET /_search?size=10&from=10
第三页 GET /_search?size=10&from=20
			]]>
			</screen>
		</section>
		
	</section>
	<section id="dsl">
		<title>Query DSL</title>
		<section id="match">
			<title>match 匹配</title>
			<screen>
curl -XGET 'http://localhost:9200/information/news/_search?pretty' -d '
{
	"query" : {
		"match" : {
			"tag" : "美"   
		}
	}
}
'
			</screen>
			
		</section>
		<section id="multi_match">
			<title>multi_match 多字段匹配</title>
			<para>multi_match 实现多字段查询</para>
			<screen>
curl -XGET 'http://localhost:9200/information/news/_search?pretty' -d '
{
	"query": {
	    "multi_match": {
		    "query":      "国际",
		    "type":       "cross_fields",
		    "fields":     [ "title", "content" ],
		    "operator":   "and"
	    }
	},
	"from": 0,
	"size": 20,
	"_source":["id","title","ctime"],
	"sort": [
	   {
	      "ctime": {"order": "desc"}
	   }
	]
	
}
'
			</screen>	
		</section>	
		
		<section id="bool">
			<title>Query bool 布尔条件</title>
			<para>Elasticsearch 提供三个布尔条件</para>
			<para>must： AND </para>
			<para>must_not：NOT</para>
			<para>should：OR</para>
			<section>
				<title>must</title>
				<para>查询必须满足 tags=天气 and title 包含 台风关键字</para>
				<screen>
				<![CDATA[
curl -XPOST http://test:123456@so.netkiller.cn/information/article/_search?pretty  -d'
{
  "query": {
    "bool": {
      "must": [
        { "match": { "tags" : "天气" }},
        { "match": { "title": "台风"   }}
      ]
    }
  },
 "_source":["id","title","ctime"],
 "highlight" : {
        "pre_tags" : ["<strong>", "<b>"],
        "post_tags" : ["</strong >", "</b>"],
        "fields" : {
            "content" : {}
        }
    }
}'
				]]>
				</screen>
				
			</section>
			<section>
				<title>should</title>
				<para>查询必须满足标title or author 两个条件</para>
				<screen>
GET /_search
{
  "query": {
    "bool": {
      "should": [
        { "match": { "title":  "Linux" }},
        { "match": { "author": "Neo"   }}
      ]
    }
  }
}
				</screen>
				<para>可以嵌套使用</para>
				<screen>
GET /_search
{
  "query": {
    "bool": {
      "should": [
        { "match": { "title":  "War and Peace" }},
        { "match": { "author": "Leo Tolstoy"   }},
        { "bool":  {
          "should": [
            { "match": { "translator": "Constance Garnett" }},
            { "match": { "translator": "Louise Maude"      }}
          ]
        }}
      ]
    }
  }
}				
				</screen>
			</section>
			<section>
				<title>must_not</title>
			</section>
		</section>
		<section id="filter">
			<title>filter 过滤</title>
			<para>query 相当于 SQL 中的 LIKE 匹配， filter 更像是 where 条件。下面的例子查询 site_id = 23 的数据并且 tags 包含 “头条” 关键字</para>
			<screen>
curl -XGET 'http://test:123456@so.netkiller.cn/information/article/_search?pretty' -d '
{
  "query": {
    "bool": {
      "must": {
        "match": {
          "tags": "头条"
        }
      },
      "filter": {
        "term": {
          "site_id" : "23"
        }
      }
    }
  }
}'
			</screen>
		</section>
		<section id="sort">
			<title>sort 排序</title>
		
			<screen>
curl -XGET 'http://localhost:9200/information/news/_search?pretty' -d '
{
	"query" : {
		"match" : {"tag" : "美"}
	},
	"sort": {
		"ctime": {"order": "desc", "mode":  "min"}
	}
}
'			</screen>
		</section>
		<section id="_source">
			<title>_source</title>
			<screen>
curl -XGET 'http://localhost:9200/information/news/_search?pretty' -d '
{
	"query" : {
		"match" : {
			"tag" : "美"   
		}
	},
	"_source":["id","title","ctime"]
}
'

curl -XGET 'http://localhost:9200/information/news/_search?pretty' -d '
{
	"_source":["id","title","ctime"],
	"query" : {
		"match" : {"tag" : "美"}
	},
	"sort": {
		"ctime": {"order": "desc", "mode":  "min"}
	}
}
'	
			</screen>
		</section>
		<section id="highlight">
			<title>highlight 高亮处理</title>
			<screen>
			<![CDATA[
curl -XPOST http://test:123456@so.netkiller.cn/information/article/_search  -d'
{
    "query" : { "match" : {  "content" : "股市" }},
    "highlight" : {
        "pre_tags" : ["<strong>", "<b>"],
        "post_tags" : ["</strong >", "</b>"],
        "fields" : {
            "content" : {}
        }
    }
}
'			
			]]>
			</screen>
		</section>
	</section>
	<section id="manager">
		<title>集群管理</title>
		<para>查看节点信息</para>
		<screen>
		<![CDATA[
root@netkiller /var/log/elasticsearch % curl -XGET localhost:9200/
{
  "name" : "node-1",
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "I7jirJ2yTr-f2qrBNorQYA",
  "version" : {
    "number" : "5.6.0",
    "build_hash" : "781a835",
    "build_date" : "2017-09-07T03:09:58.087Z",
    "build_snapshot" : false,
    "lucene_version" : "6.6.0"
  },
  "tagline" : "You Know, for Search"
}
		
		]]>
		</screen>
		
		<section id="health">
			<title>节点健康状态</title>
			<screen>
root@netkiller ~ % curl 'http://localhost:9200/_cat/health?v' 
epoch      timestamp cluster        status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent
1502445967 18:06:07  my-application yellow          2         2     17  11    0    0        5             0                  -                 77.3%			
			</screen>
			<screen>
root@netkiller ~ % curl 'http://localhost:9200/_cluster/health'  
{"cluster_name":"my-application","status":"yellow","timed_out":false,"number_of_nodes":2,"number_of_data_nodes":2,"active_primary_shards":11,"active_shards":17,"relocating_shards":0,"initializing_shards":0,"unassigned_shards":5,"delayed_unassigned_shards":0,"number_of_pending_tasks":0,"number_of_in_flight_fetch":0,"task_max_waiting_in_queue_millis":0,"active_shards_percent_as_number":77.27272727272727} 			
			</screen>
		</section>
		<section id="nodes">
			<title>节点http状态</title>
			<screen>
root@netkiller ~ % curl 'localhost:9200/_nodes/stats/http?pretty'
{
  "_nodes" : {
    "total" : 2,
    "successful" : 2,
    "failed" : 0
  },
  "cluster_name" : "my-application",
  "nodes" : {
    "-lnKCmBXRpiwExLns0jc9g" : {
      "timestamp" : 1502446878773,
      "name" : "node-1",
      "transport_address" : "10.104.3.2:9300",
      "host" : "10.104.3.2",
      "ip" : "10.104.3.2:9300",
      "roles" : [
        "master",
        "data",
        "ingest"
      ],
      "http" : {
        "current_open" : 4,
        "total_opened" : 29
      }
    },
    "WVsgYi2HT8GWnZU1kUwFwA" : {
      "timestamp" : 1502446878782,
      "name" : "node-2",
      "transport_address" : "10.186.7.221:9300",
      "host" : "10.186.7.221",
      "ip" : "10.186.7.221:9300",
      "roles" : [
        "master",
        "data",
        "ingest"
      ],
      "http" : {
        "current_open" : 0,
        "total_opened" : 2
      }
    }
  }
}
			
			</screen>
		</section>
		<section id="master">
			<title>查看master节点</title>
			<screen>
root@netkiller ~ % curl 'http://localhost:9200/_cat/nodes?v'      
ip           heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name
10.104.3.2             31          98  50    0.20    0.55     0.61 mdi       -      node-1
10.186.7.221           25          99   0    0.00    0.01     0.05 mdi       *      node-2
			</screen>
		</section>
		<section id="shards">
			<title>查看索引的节点分布</title>
			<screen>
root@netkiller ~ % curl 'http://localhost:9200/_cat/shards?v'
index        shard prirep state      docs   store ip           node
logstash-api 1     p      STARTED     103   342kb 10.186.7.221 node-2
logstash-api 1     r      STARTED     103   342kb 10.104.3.2   node-1
logstash-api 3     p      STARTED     104 404.1kb 10.186.7.221 node-2
logstash-api 3     r      STARTED     104 404.1kb 10.104.3.2   node-1
logstash-api 4     p      STARTED     117 349.2kb 10.186.7.221 node-2
logstash-api 4     r      STARTED     117 349.2kb 10.104.3.2   node-1
logstash-api 2     p      STARTED     113 405.8kb 10.186.7.221 node-2
logstash-api 2     r      STARTED     113 405.8kb 10.104.3.2   node-1
logstash-api 0     p      STARTED     128 488.4kb 10.186.7.221 node-2
logstash-api 0     r      STARTED     128 488.4kb 10.104.3.2   node-1
.kibana      0     p      STARTED       5  31.2kb 10.186.7.221 node-2
.kibana      0     r      STARTED       5  31.2kb 10.104.3.2   node-1
information  1     p      STARTED      10 122.3kb 10.104.3.2   node-1
information  1     r      UNASSIGNED                           
information  3     p      STARTED       7 159.6kb 10.104.3.2   node-1
information  3     r      UNASSIGNED                           
information  4     p      STARTED       8  86.1kb 10.104.3.2   node-1
information  4     r      UNASSIGNED                           
information  2     p      STARTED      11   160kb 10.104.3.2   node-1
information  2     r      UNASSIGNED                           
information  0     p      STARTED       9 202.8kb 10.104.3.2   node-1
information  0     r      UNASSIGNED                           
			</screen>
		</section>
		<section>
			<title>索引的开启与关闭</title>
			<screen>
POST /{index}/_close		关闭索引
POST /{index}/_open		打开索引                                                                                                                                                                      
			</screen>
			<section>
				<title>_open</title>
				<screen>
root@netkiller /var/log/elasticsearch % curl 'http://localhost:9200/_cat/indices?v'   
health status index           uuid                   pri rep docs.count docs.deleted store.size pri.store.size
       close  information     oygxIi-dR1eB9NoIZtJrxQ                                                          
green  open   logstash-spring 9xXtt_L0QvKHibXFH5gjJQ   5   1       4622            0     62.9mb         31.4mb
green  open   .kibana         9jBBaOomTO2EakZlZqnE-g   1   1         10            0     36.6kb         18.3kb				
				</screen>
				<screen>
root@netkiller /var/log/elasticsearch % curl -XPOST 'http://localhost:9200/information/_open'
{"acknowledged":true}#  				
				</screen>
				<screen>
root@netkiller /var/log/elasticsearch % curl 'http://localhost:9200/_cat/indices?v'         
health status index           uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   information     oygxIi-dR1eB9NoIZtJrxQ   5   1       2417            5     34.5mb         17.3mb
green  open   logstash-spring 9xXtt_L0QvKHibXFH5gjJQ   5   1       4622            0     62.9mb         31.4mb
green  open   .kibana         9jBBaOomTO2EakZlZqnE-g   1   1         10            0     36.6kb         18.3kb					
				</screen>
			</section>
			<section>
				<title>_close</title>
				<screen>
root@netkiller /var/log/elasticsearch % curl -XPOST 'http://localhost:9200/information/_close'
{"acknowledged":true}#  				
				</screen>
			</section>
		</section>
	</section><section id="elasticsearch-analysis-ik">
		<title>中文分词插件管理</title>
		<section>
			<title>通过 elasticsearch-plugin 命令安装分词插件</title>
			<screen>
root@netkiller ~ % /usr/share/elasticsearch/bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v5.5.1/elasticsearch-analysis-ik-5.5.1.zip
-> Downloading https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v5.5.1/elasticsearch-analysis-ik-5.5.1.zip
[=================================================] 100%   
-> Installed analysis-ik
			</screen>
			<para>创建 mapping</para>
			<screen>
root@netkiller ~ % curl -XPUT http://localhost:9200/information
			
root@netkiller ~ % curl -XPOST http://localhost:9200/information/article/_mapping -d'
{
        "properties": {
            "content": {
                "type": "text",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word"
            },
            "title": {
                "type": "text",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word"
            }
        }
}'

root@netkiller ~ % curl "http://localhost:9200/information/article/_mapping?pretty"
{
  "information" : {
    "mappings" : {
      "article" : {
        "properties" : {
          "content" : {
            "type" : "text",
            "analyzer" : "ik_max_word"
          },
          "title" : {
            "type" : "text",
            "analyzer" : "ik_max_word"
          }
        }
      }
    }
  }
}

			</screen>
		</section>
		<section id="plugin.install">
			<title>手工安装插件</title>
			<screen>
curl -s https://raw.githubusercontent.com/oscm/shell/master/search/elasticsearch/elasticsearch-analysis-ik-5.5.0.sh | bash
			</screen>
		</section>
		<section id="plugin.index">
			<title>创建索引</title>
			<screen>
curl -XPUT http://localhost:9200/information
			</screen>
		</section>
		<section id="plugin.delete">
			<title>删除索引</title>
			<para>如果索引已经存在请删除后重新创建索引</para>
			<screen>
curl -XDELETE http://localhost:9200/information/news/_mapping?pretty
curl -XDELETE http://localhost:9200/information/?pretty			
			</screen>
		</section>
		<section id="plugin.config">
			<title>配置索引分词插件</title>
			<para></para>
			<screen>
			<![CDATA[

curl -XPOST http://localhost:9200/information/news/_mapping?pretty -d'
{
    "news": {
            "_all": {
            "analyzer": "ik_max_word",
            "search_analyzer": "ik_max_word",
            "term_vector": "no",
            "store": "false"
        },
        "properties": {
            "content": {
                "type": "text",
                "store": "no",
                "term_vector": "with_positions_offsets",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word",
                "include_in_all": "true",
                "boost": 8
            }
        }
    }
}'
			]]>
			</screen>
			<section>
				<title>测试分词效果</title>
				<screen>
				<![CDATA[
curl -XPOST http://localhost:9200/information/news/ -d'
{"title": "越南胡志明游记·教堂·管风琴的天籁之音","content":"这是我平生第一次去教堂，也是第一次完整的参加宗教仪式。当我驻足教堂外的时候，耳边传来天籁之音，是管风琴，确切的说是电子风琴。真正的管风琴造价昂贵，管风琴通常需要根据教堂尺寸定制，无法量产。我记得中国只有4座管风琴，深圳音乐厅有一座。"}
'
curl -XPOST http://localhost:9200/information/news/ -d'
{"title": "越南胡志明游记·信仰·法事","content":"佛经的形成过程是与佛教的发展相始终的，按照佛教发展的时间顺序，最早形成的是小乘佛教三藏，之后形成的是大乘佛教三藏，最后形成的是密宗三藏。"}
'

curl -XPOST http://localhost:9200/information/news/_search  -d'
{
    "query" : { "term" : { "content" : "佛经" }},
    "highlight" : {
        "pre_tags" : ["<strong>", "<strong>"],
        "post_tags" : ["</strong>", "</strong>"],
        "fields" : {
            "content" : {}
        }
    }
}'		

curl -XPOST http://localhost:9200/information/news/_search  -d'
{
    "query" : { "term" : { "content" : "中国" }},
    "highlight" : {
        "pre_tags" : ["<b>", "<i>"],
        "post_tags" : ["</b>", "</i>"],
        "fields" : {
            "content" : {}
        }
    }
}'					
				]]>
				</screen>
			</section>
		</section>
		
	</section>
	<section id="index">
		<title>索引管理</title>
		<section id="index.indices">
			<title>查看索引</title>
			<screen>
root@netkiller ~ % curl 'http://localhost:9200/_cat/indices?v'         
health status index        uuid                   pri rep docs.count docs.deleted store.size pri.store.size
yellow open   information  oygxIi-dR1eB9NoIZtJrxQ   5   1         45           42      731kb          731kb
green  open   .kibana      9jBBaOomTO2EakZlZqnE-g   1   1          5            1     62.5kb         31.2kb
green  open   logstash-api WHXZhn3vRWiuVbhR8rGoEg   5   1        565            0      3.8mb          1.9mb			
			</screen>
		</section>
		<section id="index.delete">
			<title>删除索引</title>
			<screen>
			<![CDATA[
[root@netkiller logstash]# curl -XDELETE http://localhost:9200/logstash-api-2017.10.03
			]]>
			</screen>
		</section>
	</section>
	<section id="mapping">
		<title>映射</title>
		<section id="mapping.get">
			<title>查看 _mapping</title>
			<screen>
curl -XGET http://localhost:9200/information/news/_mapping?pretty		
			</screen>
			<para>数据结构如下</para>
			<screen>
{
  "information" : {
    "mappings" : {
      "news" : {
        "_all" : {
          "analyzer" : "ik_max_word"
        },
        "properties" : {
          "content" : {
            "type" : "string",
            "boost" : 8.0,
            "term_vector" : "with_positions_offsets",
            "analyzer" : "ik_max_word",
            "include_in_all" : true
          },
          "ctime" : {
            "type" : "string"
          },
          "division_category_id" : {
            "type" : "long"
          },
          "tag" : {
            "type" : "string",
            "boost" : 8.0,
            "term_vector" : "with_positions_offsets",
            "analyzer" : "ik_max_word",
            "include_in_all" : true
          },
          "title" : {
            "type" : "string",
            "boost" : 8.0,
            "term_vector" : "with_positions_offsets",
            "analyzer" : "ik_max_word",
            "include_in_all" : true
          }
        }
      }
    }
  }
}
			</screen>	
		</section>
		<section id="mapping.delete">
			<title>删除 _mapping </title>
			<screen>
curl -XDELETE http://localhost:9200/information/news/_mapping?pretty		
			</screen>
		</section>
		<section id="mapping.post">
			<title>创建 _mapping</title>
			<screen>
curl -XPOST http://localhost:9200/information/news/_mapping?pretty -d'
{
    "news": {
        "_all": {
	       "analyzer": "ik_max_word",
	       "search_analyzer": "ik_max_word",
	       "term_vector": "no",
	       "store": "false"
        },
        "properties": {
            "content": {
                "type": "string",
                "store": "no",
                "term_vector": "with_positions_offsets",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word",
                "include_in_all": "true",
                "boost": 8
            }
        }
    }
}'
			</screen>
		</section>
		<section id="mapping.update">
			<title>更新 mapping</title>
			<para>注意：更新只能用于空的index，如果index中存在数据无法修改_mapping，必须重建，或者采用别名方案</para>
			<para>更新已存在的 mapping，首先我们创建一个 _mapping</para>
			<screen>
			<![CDATA[
% curl "http://localhost:9200/information/article/_mapping?pretty"
{
  "information" : {
    "mappings" : {
      "article" : {
        "properties" : {
          "content" : {
            "type" : "text",
            "analyzer" : "ik_max_word"
          },
          "title" : {
            "type" : "text",
            "analyzer" : "ik_max_word"
          }
        }
      }
    }
  }
}

			]]>
			</screen>
			<para>在这个 _mapping 中增加 ctime 字段，定义时间格式为 yyyy-MM-dd HH:mm:ss</para>
			<screen>
			<![CDATA[
% curl -XPOST http://localhost:9200/information/article/_mapping -d'
{
        "properties": {
        "ctime": {
               "type":   "date",
               "format": "yyyy-MM-dd HH:mm:ss"
           }
        }
}'

			]]>
			</screen>
			<para>查看预期结果</para>
			<screen>
			<![CDATA[
% curl "http://localhost:9200/information/article/_mapping?pretty"  
{
  "information" : {
    "mappings" : {
      "article" : {
        "properties" : {
          "content" : {
            "type" : "text",
            "analyzer" : "ik_max_word"
          },
          "ctime" : {
            "type" : "date",
            "format" : "yyyy-MM-dd HH:mm:ss"
          },
          "title" : {
            "type" : "text",
            "analyzer" : "ik_max_word"
          }
        }
      }
    }
  }
}

			]]>
			</screen>
		</section>
		<section id="mapping.change">
			<title>修改 _mapping</title>
			<para>修改流程需要经历五步，首先创建新索引，创建新_mapping，导入数据，索引别名，删除旧索引。</para>
			<para>当然你也可以删除重建索引，为什么会这么折腾呢？因为这样不用停止业务的情况下进行迁移。</para>
			<screen>
			<![CDATA[
# curl -XGET http://localhost:9200/information_v1/news/_mapping?pretty
{
  "information_v1" : {
    "mappings" : {
      "news" : {
        "_all" : {
          "analyzer" : "ik_max_word"
        },
        "properties" : {
          "content" : {
            "type" : "string",
            "boost" : 8.0,
            "term_vector" : "with_positions_offsets",
            "analyzer" : "ik_max_word",
            "include_in_all" : true
          },
          "ctime" : {
            "type" : "string"
          },
          "division_category_id" : {
            "type" : "long"
          },
          "tag" : {
            "type" : "string",
            "boost" : 8.0,
            "term_vector" : "with_positions_offsets",
            "analyzer" : "ik_max_word",
            "include_in_all" : true
          },
          "title" : {
            "type" : "string",
            "boost" : 8.0,
            "term_vector" : "with_positions_offsets",
            "analyzer" : "ik_max_word",
            "include_in_all" : true
          }
        }
      }
    }
  }
}

			]]>
			</screen>
			<para>注意 ctime 数据类型定义错误，现在需要将它改为date日期类型。</para>
			<para>创建 information_v2 索引</para>
			<screen>
			<![CDATA[
curl -XPUT http://localhost:9200/information_v2
curl -XPOST http://localhost:9200/information_v2/news/_mapping?pretty -d'
{
    "news": {
            "_all": {
            "analyzer": "ik_max_word",
            "search_analyzer": "ik_max_word",
            "term_vector": "no",
            "store": "false"
        	},
		"properties": {
			"title": {
                "type": "string",
                "store": "no",
                "term_vector": "with_positions_offsets",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word",
                "include_in_all": "true",
                "boost": 8
			},
			"content": {
                "type": "string",
                "store": "no",
                "term_vector": "with_positions_offsets",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word",
                "include_in_all": "true",
                "boost": 8
			},
			"tag": {
                "type": "string",
                "store": "no",
                "term_vector": "with_positions_offsets",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word",
                "include_in_all": "true",
                "boost": 8
            },
            "ctime": { 
				"type": "date"
		    }
        }
    }
}'	
			]]>
			</screen>
			<para>查看全新 _mapping</para>
			<screen>
			<![CDATA[
# curl -XGET http://localhost:9200/information_v2/news/_mapping?pretty
{
  "information_v2" : {
    "mappings" : {
      "news" : {
        "_all" : {
          "analyzer" : "ik_max_word"
        },
        "properties" : {
          "content" : {
            "type" : "string",
            "boost" : 8.0,
            "term_vector" : "with_positions_offsets",
            "analyzer" : "ik_max_word",
            "include_in_all" : true
          },
          "ctime" : {
            "type" : "date",
            "format" : "strict_date_optional_time||epoch_millis"
          },
          "tag" : {
            "type" : "string",
            "boost" : 8.0,
            "term_vector" : "with_positions_offsets",
            "analyzer" : "ik_max_word",
            "include_in_all" : true
          },
          "title" : {
            "type" : "string",
            "boost" : 8.0,
            "term_vector" : "with_positions_offsets",
            "analyzer" : "ik_max_word",
            "include_in_all" : true
          }
        }
      }
    }
  }
}
			]]>
			</screen>
			<para>现在导入数据，导入完成后修改别名，将information 从 information_v1 切换到 information_v2</para>
			<screen>
			<![CDATA[
curl -XPOST http://localhost:9200/_aliases -d '
{
    "actions": [
        { "remove": {
            "alias": "information",
            "index": "information_v1"
        }},
        { "add": {
            "alias": "information",
            "index": "information_v2"
        }}
    ]
}
'
			]]>
			</screen>
			<para>当所以切换完成information_v1 已经没有什么用处了，这时可以删除information_v1</para>
			<screen>
			<![CDATA[
curl -XDELETE http://localhost:9200/information_v1
			]]>
			</screen>
		</section>
		<section id="type">
			<title>数据类型</title>
			<para>string, date, long, double, boolean or ip.</para>
			<section>
				<title>date</title>
				<para>elasticsearch 采用 ISO 8601 标准的 date 格式</para>
				<screen>
				<![CDATA[
{"LastUpdate": {
    "type" : "date",
    "format" : "yyyy-MM-dd HH:mm:ss"}
}				
				]]>
				</screen>
				<screen>
				<![CDATA[
{
  "mappings": {
    "my_type": {
      "properties": {
        "date": {
          "type":   "date",
          "format": "yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis"
        }
      }
    }
  }
}				
				]]>
				</screen>
			</section>
		</section>
	</section>
	<section id="alias">
		<title>Alias management 别名管理</title>
		<section>
			<title>查看索引别名</title>
			<para>没有设置任何别名将返回下面的数据结构</para>
			<screen>
# curl -XGET http://localhost:9200/_aliases?pretty
{
  "information_v1" : {
    "aliases" : { }
  },
  "information_v2" : {
    "aliases" : { }
  }
}
			</screen>
			<para>information 是 information_v1 的别名</para>
			<screen>
# curl -XGET http://localhost:9200/_aliases?pretty
{
  "information_v1" : {
    "aliases" : {
      "information" : { }
    }
  },
  "information_v2" : {
    "aliases" : { }
  }
}
			</screen>
		</section>
		<section>
			<title>创建索引别名</title>
			<screen>
curl -XPUT http://localhost:9200/information_v1
curl -XPOST http://localhost:9200/information_v1/news/_mapping?pretty -d'
{
    "news": {
            "_all": {
            "analyzer": "ik_max_word",
            "search_analyzer": "ik_max_word",
            "term_vector": "no",
            "store": "false"
        	},
		"properties": {
			"title": {
                "type": "string",
                "store": "no",
                "term_vector": "with_positions_offsets",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word",
                "include_in_all": "true",
                "boost": 8
			},
			"content": {
                "type": "string",
                "store": "no",
                "term_vector": "with_positions_offsets",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word",
                "include_in_all": "true",
                "boost": 8
			},
			"tag": {
                "type": "string",
                "store": "no",
                "term_vector": "with_positions_offsets",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word",
                "include_in_all": "true",
                "boost": 8
            },
            "ctime": { 
				"type": "date"
		    }
        }
    }
}'	
			</screen>
			<screen>
curl -XPOST http://localhost:9200/_aliases -d '
{
    "actions": [
        { "add": {
            "alias": "information",
            "index": "information_v1"
        }}
    ]
}
'

{"acknowledged":true}

			</screen>
			<para>查看结果</para>
			<screen>
# curl -XGET http://localhost:9200/_aliases?pretty
{
  "information_v1" : {
    "aliases" : {
      "information" : { }
    }
  },
  "information_v2" : {
    "aliases" : { }
  }
}


# curl -XGET http://localhost:9200/information/?pretty
{
  "information_v1" : {
    "aliases" : {
      "information" : { }
    },
    "mappings" : {
      "news" : {
        "_all" : {
          "analyzer" : "ik_max_word"
        },
        "properties" : {
          "content" : {
            "type" : "string",
            "boost" : 8.0,
            "term_vector" : "with_positions_offsets",
            "analyzer" : "ik_max_word",
            "include_in_all" : true
          },
          "ctime" : {
            "type" : "date",
            "format" : "strict_date_optional_time||epoch_millis"
          },
          "tag" : {
            "type" : "string",
            "boost" : 8.0,
            "term_vector" : "with_positions_offsets",
            "analyzer" : "ik_max_word",
            "include_in_all" : true
          },
          "title" : {
            "type" : "string",
            "boost" : 8.0,
            "term_vector" : "with_positions_offsets",
            "analyzer" : "ik_max_word",
            "include_in_all" : true
          }
        }
      }
    },
    "settings" : {
      "index" : {
        "creation_date" : "1471929807430",
        "number_of_shards" : "5",
        "number_of_replicas" : "1",
        "uuid" : "gWl8TTT-QnKbKj2BglfG-w",
        "version" : {
          "created" : "2030599"
        }
      }
    },
    "warmers" : { }
  }
}
			
			</screen>
		</section>

		<section>
			<title>修改别名</title>
			<screen>
			<![CDATA[
curl -XPOST http://localhost:9200/_aliases -d '
{
    "actions": [
        { "remove": {
            "alias": "information",
            "index": "information_v1"
        }},
        { "add": {
            "alias": "information",
            "index": "information_v2"
        }}
    ]
}
'
			]]>
			</screen>			
		</section>
		<section>
			<title>删除别名</title>
			<screen>
			<![CDATA[
curl -XPOST http://localhost:9200/_aliases -d '
{
    "actions": [
        { "remove": {
            "alias": "information","index": "information_v2"
        }}
    ]
}
'
			]]>
			</screen>
		</section>
	</section>

	<section id="example">
		<title>Example</title>
	
		<section>
			<title>新闻资讯应用案例</title>
			<screen>
			<![CDATA[
curl -XDELETE http://localhost:9200/information_v1/news/_mapping?pretty
curl -XDELETE http://localhost:9200/information_v1/?pretty

curl -XPUT http://localhost:9200/information_v1

curl -XPOST http://localhost:9200/information_v1/news/_mapping?pretty -d'
{
    "news": {
            "_all": {
            "analyzer": "ik_max_word",
            "search_analyzer": "ik_max_word",
            "term_vector": "no",
            "store": "false"
        	},
		"properties": {
			"id": { 
				"type": "long"
		    },
			"title": {
                "type": "string",
                "store": "no",
                "term_vector": "with_positions_offsets",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word",
                "include_in_all": "true",
                "boost": 8
			},
			"content": {
                "type": "string",
                "store": "no",
                "term_vector": "with_positions_offsets",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word",
                "include_in_all": "true",
                "boost": 8
			},
			"tag": {
                "type": "string",
                "store": "no",
                "term_vector": "with_positions_offsets",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word",
                "include_in_all": "true",
                "boost": 8
            },
            "ctime": { 
				"type": "date"
		    }
        }
    }
}'

curl -XPOST http://localhost:9200/_aliases -d '
{
    "actions": [
        { "add": {
            "alias": "information",
            "index": "information_v1"
        }}
    ]
}
'

curl -XGET http://localhost:9200/information/?pretty


curl -XPOST 'http://localhost:9200/information/news/1?pretty' -d '{
	"id":1,
	"title":"新闻标题",
	"content":"新闻内容",
	"tag":"新闻标签",
	"ctime":"2011-11-11T11:11:11"
}'

# curl -XGET 'http://localhost:9200/information/news/1?pretty'
{
  "_index" : "information_v1",
  "_type" : "news",
  "_id" : "1",
  "_version" : 1,
  "found" : true,
  "_source" : {
    "id" : 1,
    "title" : "新闻标题",
    "content" : "新闻内容",
    "tag" : "新闻标签",
    "ctime" : "2011-11-11T11:11:11"
  }
}

curl -XPOST http://localhost:9200/information/news/_search?pretty  -d'
{
    "query" : { "term" : { "content" : "新闻" }},
    "highlight" : {
        "pre_tags" : ["<b>", "<b>"],
        "post_tags" : ["</b>", "</b>"],
        "fields" : {
            "content" : {}
        }
    }
}'

curl -XPOST http://localhost:9200/information/news/_search  -d'
{
    "query" : { "term" : { "content" : "王宝强" }},
    "highlight" : {
        "pre_tags" : ["<b>", "<b>"],
        "post_tags" : ["</b>", "</b>"],
        "fields" : {
            "content" : {}
        }
    }
}'


curl -XPOST http://localhost:9200/information/news/_search  -d'
{
    "query" : { "term" : { "tag" : "娱乐" }},
    "highlight" : {
        "pre_tags" : ["<b>", "<b>"],
        "post_tags" : ["</b>", "</b>"],
        "fields" : {
            "tag" : {}
        }
    }
}'


			]]>
			</screen>
		</section>
		<section>
			<title>文章搜索案例</title>
			<screen>
			<![CDATA[
curl -XDELETE http://localhost:9200/information

curl -XPUT http://localhost:9200/information
			
curl -XPOST http://localhost:9200/information/article/_mapping -d'
{
        "properties": {
            "title": {
                "type": "text",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word"
            },
            "description": {
                "type": "text",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word"
            },
            "content": {
                "type": "text",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word"
            },
            "ctime": {
               "type":   "date",
               "format": "yyyy-MM-dd HH:mm:ss"
           	},
 			"mtime": {
               "type":   "date",
               "format": "yyyy-MM-dd HH:mm:ss"
           	}
        }
}'

curl "http://localhost:9200/information/article/_mapping?pretty"

			]]>
			</screen>
		</section>
	</section>
	<section id="jdbc-input-plugin">
		<title>Migrating MySQL Data into Elasticsearch using logstash</title>
		<para>https://www.elastic.co/guide/en/logstash/current/plugins-inputs-jdbc.html</para>
		<section id="logstash.install">
			<title>安装 logstash</title>
			<para>安装 JDBC 驱动 和 Logstash </para>
			<screen>
curl -s https://raw.githubusercontent.com/oscm/shell/master/database/mysql/5.7/mysql-connector-java.sh	 | bash			
curl -s https://raw.githubusercontent.com/oscm/shell/master/search/logstash/logstash-5.x.sh | bash
			</screen>
			<para>mysql 驱动文件位置在 /usr/share/java/mysql-connector-java.jar </para>
		</section>
		<section id="logstash.config">
			<title>配置 logstash</title>
			<para>创建配置文件 /etc/logstash/conf.d/jdbc-mysql.conf</para>
			<screen>
			<![CDATA[
mysql> desc article;
+-------------+--------------+------+-----+---------+-------+
| Field       | Type         | Null | Key | Default | Extra |
+-------------+--------------+------+-----+---------+-------+
| id          | int(11)      | NO   |     | 0       |       |
| title       | mediumtext   | NO   |     | NULL    |       |
| description | mediumtext   | YES  |     | NULL    |       |
| author      | varchar(100) | YES  |     | NULL    |       |
| source      | varchar(100) | YES  |     | NULL    |       |
| ctime       | datetime     | NO   |     | NULL    |       |
| content     | longtext     | YES  |     | NULL    |       |
+-------------+--------------+------+-----+---------+-------+
7 rows in set (0.00 sec)
			]]>
			</screen>
			<screen>
			<![CDATA[
input {
  jdbc {
    jdbc_driver_library => "/usr/share/java/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/cms"
    jdbc_user => "cms"
    jdbc_password => "password"
    schedule => "* * * * *"
    statement => "select * from article"
  }
}
output {
    elasticsearch {
    		hosts => "localhost:9200"
        index => "information"
        document_type => "article"
        document_id => "%{id}"
        
    }
}
			]]>
			</screen>
		</section>
		<section id="logstash.start">
			<title>启动 Logstash</title>
			<screen>
			<![CDATA[
root@netkiller /var/log/logstash % systemctl restart logstash

root@netkiller /var/log/logstash % systemctl status logstash
● logstash.service - logstash
   Loaded: loaded (/etc/systemd/system/logstash.service; enabled; vendor preset: disabled)
   Active: active (running) since Mon 2017-07-31 09:35:00 CST; 11s ago
 Main PID: 10434 (java)
   CGroup: /system.slice/logstash.service
           └─10434 /usr/bin/java -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+DisableExplicitGC -Djava.awt.headless=true -Dfi...

Jul 31 09:35:00 netkiller systemd[1]: Started logstash.
Jul 31 09:35:00 netkiller systemd[1]: Starting logstash...
			
root@netkiller /var/log/logstash % cat logstash-plain.log 
[2017-07-31T09:35:28,169][INFO ][logstash.outputs.elasticsearch] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://localhost:9200/]}}
[2017-07-31T09:35:28,172][INFO ][logstash.outputs.elasticsearch] Running health check to see if an Elasticsearch connection is working {:healthcheck_url=>http://localhost:9200/, :path=>"/"}
[2017-07-31T09:35:28,298][WARN ][logstash.outputs.elasticsearch] Restored connection to ES instance {:url=>#<Java::JavaNet::URI:0x453a18e9>}
[2017-07-31T09:35:28,299][INFO ][logstash.outputs.elasticsearch] Using mapping template from {:path=>nil}
[2017-07-31T09:35:28,337][INFO ][logstash.outputs.elasticsearch] Attempting to install template {:manage_template=>{"template"=>"logstash-*", "version"=>50001, "settings"=>{"index.refresh_interval"=>"5s"}, "mappings"=>{"_default_"=>{"_all"=>{"enabled"=>true, "norms"=>false}, "dynamic_templates"=>[{"message_field"=>{"path_match"=>"message", "match_mapping_type"=>"string", "mapping"=>{"type"=>"text", "norms"=>false}}}, {"string_fields"=>{"match"=>"*", "match_mapping_type"=>"string", "mapping"=>{"type"=>"text", "norms"=>false, "fields"=>{"keyword"=>{"type"=>"keyword", "ignore_above"=>256}}}}}], "properties"=>{"@timestamp"=>{"type"=>"date", "include_in_all"=>false}, "@version"=>{"type"=>"keyword", "include_in_all"=>false}, "geoip"=>{"dynamic"=>true, "properties"=>{"ip"=>{"type"=>"ip"}, "location"=>{"type"=>"geo_point"}, "latitude"=>{"type"=>"half_float"}, "longitude"=>{"type"=>"half_float"}}}}}}}}
[2017-07-31T09:35:28,344][INFO ][logstash.outputs.elasticsearch] Installing elasticsearch template to _template/logstash
[2017-07-31T09:35:28,465][INFO ][logstash.outputs.elasticsearch] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>[#<Java::JavaNet::URI:0x66df34ae>]}
[2017-07-31T09:35:28,483][INFO ][logstash.pipeline        ] Starting pipeline {"id"=>"main", "pipeline.workers"=>8, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>5, "pipeline.max_inflight"=>1000}
[2017-07-31T09:35:29,562][INFO ][logstash.pipeline        ] Pipeline main started
[2017-07-31T09:35:29,700][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2017-07-31T09:36:01,019][INFO ][logstash.inputs.jdbc     ] (0.006000s) select * from article	
			]]>
			</screen>
		</section>
		<section id="logstash.test">
			<title>验证</title>
			<screen>
			<![CDATA[
% curl -XGET 'http://localhost:9200/_all/_search?pretty'
			]]>
			</screen>
		</section>
		<section id="logstash.template">
			<title>配置模板</title>
			<section>
				<title>全量导入</title>
				<para>适合数据没有改变的归档数据或者只能增加没有修改的数据</para>
				<screen>
				<![CDATA[
input {
  jdbc {
    jdbc_driver_library => "/usr/share/java/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/cms"
    jdbc_user => "cms"
    jdbc_password => "password"
    schedule => "* * * * *"
    statement => "select * from article"
  }
}
output {
    elasticsearch {
    		hosts => "localhost:9200"
        index => "information"
        document_type => "article"
        document_id => "%{id}"
        
    }
}
				]]>
				</screen>
			</section>
			<section>
				<title>多表导入</title>
				<para>多张数据表导入到 Elasticsearch</para>
				<screen>
				<![CDATA[
# multiple inputs on logstash jdbc

input {
  jdbc {
    jdbc_driver_library => "/usr/share/java/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/cms"
    jdbc_user => "cms"
    jdbc_password => "password"
    schedule => "* * * * *"
    statement => "select * from article"
    type => "article"
  }
  jdbc {
    jdbc_driver_library => "/usr/share/java/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/cms"
    jdbc_user => "cms"
    jdbc_password => "password"
    schedule => "* * * * *"
    statement => "select * from comment"
    type => "comment"
  } 
}
output {
    elasticsearch {
    		hosts => "localhost:9200"
        index => "information"
        document_type => "%{type}"
        document_id => "%{id}"
        
    }
}				
				]]>
				</screen>
				<para>需要在每一个jdbc配置项中加入 type 配置，然后 elasticsearch 配置项中加入 document_type => "%{type}" </para>
			</section>
			<section>
				<title>通过 ID 主键字段增量复制数据</title>
				<screen>
				<![CDATA[
input {
  jdbc {
    statement => "SELECT id, mycolumn1, mycolumn2 FROM my_table WHERE id > :sql_last_value"
    use_column_value => true
    tracking_column => "id"
    tracking_column_type => "numeric"
    # ... other configuration bits
  }
}
				]]>
				</screen>
				<para>tracking_column_type => "numeric" 可以声明 id 字段的数据类型, 如果不指定将会默认为日期</para>
				<screen>
[2017-07-31T11:08:00,193][INFO ][logstash.inputs.jdbc     ] (0.020000s) select * from article where id > '2017-07-31 02:47:00'
				</screen>
				<para>如果复制不对称可以加入 clean_run => true 配置项，清楚数据</para>
			</section>
			<section>
				<title>通过日期字段增量复制数据</title>
				<screen>
				<![CDATA[
input {
  jdbc {
    statement => "SELECT * FROM my_table WHERE create_date > :sql_last_value"
    use_column_value => true
    tracking_column => "create_date"
    # ... other configuration bits
  }
}
				]]>
				</screen>
				<para>如果复制不对称可以加入 clean_run => true 配置项，清楚数据</para>
			</section>
			<section>
				<title>指定SQL文件</title>
				<para>statement_filepath 指定 SQL 文件，有时SQL太复杂写入 statement 配置项维护部方便，可以将 SQL 写入一个文本文件，然后使用 statement_filepath 配置项引用该文件。</para>
				<screen>
				<![CDATA[
input {
    jdbc {
        jdbc_driver_library => "/path/to/driver.jar"
        jdbc_driver_class => "org.postgresql.Driver"
        jdbc_url => "jdbc://postgresql"
        jdbc_user => "neo"
        jdbc_password => "password"
        statement_filepath => "query.sql"
    }
}				
				]]>
				</screen>
			</section>
			<section>
				<title>参数传递</title>
				<para>将需要复制的条件参数写入 parameters 配置项</para>
				<screen>
				<![CDATA[
input {
  jdbc {
    jdbc_driver_library => "mysql-connector-java-5.1.36-bin.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/mydb"
    jdbc_user => "mysql"
    parameters => { "favorite_artist" => "Beethoven" }
    schedule => "* * * * *"
    statement => "SELECT * from songs where artist = :favorite_artist"
  }
}				
				]]>
				</screen>
			</section>
			<section>
				<title>控制返回JDBC数据量</title>
				<screen>
	jdbc_fetch_size => 1000  #jdbc获取数据的数量大小
	jdbc_page_size => 1000 #jdbc一页的大小，
	jdbc_paging_enabled => true  #和jdbc_page_size组合，将statement的查询分解成多个查询,相当于: SELECT * FROM table LIMIT 1000 OFFSET 4000 				
				</screen>
			</section>
			<section>
				<title>输出到不同的 Elasticsearch 中</title>
				<para>通过 if [type]=="news" 执行不同的区块，实现将不同的type输出到指定的 index 中。</para>
				<screen>
output {
	if [type]=="news" {
	  elasticsearch {
	  	hosts => "node1.netkiller.cn:9200"
		index => "information"
		document_id => "%{id}"
	  }
	}
	
	if [type]=="comment" {
	  elasticsearch {
		hosts => "node2.netkiller.cn:9200"
		index => "information"
		document_id => "%{id}"
	  }
	}
}		
				</screen>
			</section>
			<section>
				<title>日期格式转换</title>
				<para>日期格式化, 将ISO 8601日期格式转换为 %Y-%m-%d %H:%M:%S</para>
				<screen>
				<![CDATA[
input {
	jdbc {
		jdbc_driver_library => "/usr/share/java/mysql-connector-java.jar"
		jdbc_driver_class => "com.mysql.jdbc.Driver"
		jdbc_connection_string => "jdbc:mysql://127.0.0.1:3306/cms"
		jdbc_user => "cms"
		jdbc_password => "123456"
		schedule => "* * * * *"
		statement => "select * from article limit 5"
	}

}
filter {

	ruby {
        code => "event.set('ctime', event.get('[ctime]').time.localtime.strftime('%Y-%m-%d %H:%M:%S'))"
    }

	ruby {
        code => "event.set('mtime', event.get('[mtime]').time.localtime.strftime('%Y-%m-%d %H:%M:%S'))"
    }
}
output {

	stdout {
		codec => rubydebug
	}

}				
				]]>
				</screen>
			</section>
			<section id="logstash.example">
				<title>example</title>
				<para>下面的例子实现了新数据复制，旧数据更新</para>
				<screen>
				<![CDATA[
input {
  jdbc {
    jdbc_driver_library => "/usr/share/java/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/cms"
    jdbc_user => "cms"
    jdbc_password => "password"
    schedule => "* * * * *"	#定时cron的表达式,这里是每分钟执行一次
    statement => "select id, title, description, author, source, ctime, content from article where id > :sql_last_value"
    use_column_value => true
    tracking_column => "id"
    tracking_column_type => "numeric" 
    record_last_run => true
    last_run_metadata_path => "/var/tmp/article.last"
  }
  jdbc {
    jdbc_driver_library => "/usr/share/java/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/cms"
    jdbc_user => "cms"
    jdbc_password => "password"
    schedule => "* * * * *"	#定时cron的表达式,这里是每分钟执行一次
    statement => "select * from article where ctime > :sql_last_value"
    use_column_value => true
    tracking_column => "ctime"
    tracking_column_type => "timestamp" 
    record_last_run => true
    last_run_metadata_path => "/var/tmp/article-ctime.last"
  }
}
output {
    elasticsearch {
    		hosts => "localhost:9200"
        index => "information"
        document_type => "article"
        document_id => "%{id}"
        action => "update"　 # 操作执行的动作,可选值有["index", "delete", "create", "update"]
        doc_as_upsert => true  #支持update模式
    }
}
				]]>
				</screen>
			</section>
		</section>

		<section id="logstash.sync">
			<title>解决数据不对称问题</title>
			<para>jdbc-input-plugin 只能实现数据库的追加，对于 elasticsearch 增量写入，但经常jdbc源一端的数据库可能会做数据库删除或者更新操作。这样一来数据库与搜索引擎的数据库就出现了不对称的情况。</para>
			<para>当然你如果有开发团队可以写程序在删除或者更新的时候同步对搜索引擎操作。如果你没有这个能力，可以尝试下面的方法。</para>
			<para>这里有一个数据表 article , mtime 字段定义了 ON UPDATE CURRENT_TIMESTAMP 所以每次更新mtime的时间都会变化</para>
			<screen>
			<![CDATA[
mysql> desc article;
+-------------+--------------+------+-----+--------------------------------+-------+
| Field       | Type         | Null | Key | Default                        | Extra |
+-------------+--------------+------+-----+--------------------------------+-------+
| id          | int(11)      | NO   |     | 0                              |       |
| title       | mediumtext   | NO   |     | NULL                           |       |
| description | mediumtext   | YES  |     | NULL                           |       |
| author      | varchar(100) | YES  |     | NULL                           |       |
| source      | varchar(100) | YES  |     | NULL                           |       |
| content     | longtext     | YES  |     | NULL                           |       |
| status      | enum('Y','N')| NO   |     | 'N'                            |       |
| ctime       | timestamp    | NO   |     | CURRENT_TIMESTAMP              |       |
| mtime       | timestamp    | YES  |     | ON UPDATE CURRENT_TIMESTAMP    |       |
+-------------+--------------+------+-----+--------------------------------+-------+
7 rows in set (0.00 sec)
			]]>
			</screen>
			<para>logstash 增加 mtime 的查询规则</para>
			<screen>
			<![CDATA[
  jdbc {
    jdbc_driver_library => "/usr/share/java/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/cms"
    jdbc_user => "cms"
    jdbc_password => "password"
    schedule => "* * * * *"	#定时cron的表达式,这里是每分钟执行一次
    statement => "select * from article where mtime > :sql_last_value"
    use_column_value => true
    tracking_column => "mtime"
    tracking_column_type => "timestamp" 
    record_last_run => true
    last_run_metadata_path => "/var/tmp/article-mtime.last"
  }
			
			]]>
			</screen>
			<para>创建回收站表，这个事用于解决数据库删除，或者禁用 status = 'N' 这种情况的。</para>
			<screen>
			<![CDATA[
CREATE TABLE `elasticsearch_trash` (
  `id` int(11) NOT NULL,
  `ctime` timestamp NULL DEFAULT CURRENT_TIMESTAMP,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8
			]]>
			</screen>
			<para>为 article 表创建触发器</para>
			<screen>
			<![CDATA[
CREATE DEFINER=`dba`@`%` TRIGGER `article_BEFORE_UPDATE` BEFORE UPDATE ON `article` FOR EACH ROW
BEGIN
	-- 此处的逻辑是解决文章状态变为 N 的时候，需要将搜索引擎中对应的数据删除。
	IF NEW.status = 'N' THEN
		insert into elasticsearch_trash(id) values(OLD.id);
	END IF;
	-- 此处逻辑是修改状态到 Y 的时候，方式elasticsearch_trash仍然存在该文章ID，导致误删除。所以需要删除回收站中得回收记录。
    IF NEW.status = 'Y' THEN
		delete from elasticsearch_trash where id = OLD.id;
	END IF;
END

CREATE DEFINER=`dba`@`%` TRIGGER `article_BEFORE_DELETE` BEFORE DELETE ON `article` FOR EACH ROW
BEGIN
	-- 此处逻辑是文章被删除同事将改文章放入搜索引擎回收站。
	insert into elasticsearch_trash(id) values(OLD.id);
END
			]]>
			</screen>
			<para>接下来我们需要写一个简单地 Shell 每分钟运行一次，从 elasticsearch_trash 数据表中取出数据，然后使用 curl 命令调用 elasticsearch restful 接口，删除被收回的数据。 </para>
		</section>
		
		<section id="logstash.mapping">
			<title>修改 Mapping</title>
			<paraf>需求 Elasticsearch 时间格式 从ISO 8601 到 yyyy-MM-dd HH:mm:ss。首先停止 logstash</paraf>
			<screen>
			<![CDATA[
systemctl stop logstash

rm -rf /var/tmp/article* 			
			]]>
			</screen>
			<para>修改 /etc/logstash/conf.d/jdbc.conf 配置文件</para>
			<screen>
			<![CDATA[
input {
  jdbc {
    jdbc_driver_library => "/usr/share/java/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/cms"
    jdbc_user => "cms"
    jdbc_password => "123456"
    schedule => "* * * * *"
    statement => "select * from article where id > :sql_last_value"
    use_column_value => true
    tracking_column => "id"
    tracking_column_type => "numeric" 
    record_last_run => true
    last_run_metadata_path => "/var/tmp/article.last"
  }
jdbc {
    jdbc_driver_library => "/usr/share/java/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/cms"
    jdbc_user => "cms"
    jdbc_password => "123456"
    schedule => "* * * * *"	#定时cron的表达式,这里是每分钟执行一次
    statement => "select * from article where ctime > :sql_last_value"
    use_column_value => true
    tracking_column => "ctime"
    tracking_column_type => "timestamp" 
    record_last_run => true
    last_run_metadata_path => "/var/tmp/article-ctime.last"
  }

}

filter {

    ruby {
        code => "event.set('ctime', event.get('[ctime]').time.localtime.strftime('%Y-%m-%d %H:%M:%S'))"
    }

    ruby {
        code => "event.set('mtime', event.get('[mtime]').time.localtime.strftime('%Y-%m-%d %H:%M:%S'))"
    }

}

output {
    elasticsearch {
    	hosts => "localhost:9200"
        index => "information"
        document_type => "article"
        document_id => "%{id}"
        action => "update"
        doc_as_upsert => true
    }
}
			
			]]>
			</screen>
			<para>删除就的index，重新创建，并配置 mapping。</para>
			<programlisting>
			<![CDATA[


curl -XDELETE http://localhost:9200/information

curl -XPUT http://localhost:9200/information
			
curl -XPOST http://localhost:9200/information/article/_mapping -d'
{
        "properties": {
            "title": {
                "type": "text",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word"
            },
            "description": {
                "type": "text",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word"
            },
            "content": {
                "type": "text",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word"
            },
            "ctime": {
               "type":   "date",
               "format": "yyyy-MM-dd HH:mm:ss"
           	},
 			"mtime": {
               "type":   "date",
               "format": "yyyy-MM-dd HH:mm:ss"
           	}
        }
}'

curl "http://localhost:9200/information/article/_mapping?pretty"
			]]>
			</programlisting>
			<para>启动 logstash 重新复制数据。</para>
			<screen>
			<![CDATA[
rm -rf /var/log/logstash/*
systemctl start logstash			
			]]>
			</screen>
			
		</section>
		
	</section>
	&chapter.elasticsearch.old.xml;
	<section id="faq">
		<title>FAQ</title>
		<section>
			<title>Plugin [analysis-ik] is incompatible with Elasticsearch [2.3.5]. Was designed for version [2.3.4]</title>
			<screen>
			<![CDATA[
[2016-08-20 19:18:40,930][INFO ][node                     ] [Morg] version[2.3.5], pid[31494], build[90f439f/2016-07-27T10:36:52Z]
[2016-08-20 19:18:40,930][INFO ][node                     ] [Morg] initializing ...
[2016-08-20 19:18:41,360][ERROR][bootstrap                ] Exception
java.lang.IllegalArgumentException: Plugin [analysis-ik] is incompatible with Elasticsearch [2.3.5]. Was designed for version [2.3.4]
	at org.elasticsearch.plugins.PluginInfo.readFromProperties(PluginInfo.java:118)
	at org.elasticsearch.plugins.PluginsService.getPluginBundles(PluginsService.java:378)
	at org.elasticsearch.plugins.PluginsService.<init>(PluginsService.java:128)
	at org.elasticsearch.node.Node.<init>(Node.java:158)
	at org.elasticsearch.node.Node.<init>(Node.java:140)
	at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:143)
	at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:178)
	at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:270)
	at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
			]]>
			</screen>
			<para>解决方案</para>
			<screen>
cd /usr/share/elasticsearch/plugins/ik
vim plugin-descriptor.properties

elasticsearch.version=2.3.4
改为
elasticsearch.version=2.3.5
			</screen>
		</section>
		<section>
			<title>plugin [analysis-ik] is incompatible with version [5.6.1]; was designed for version [5.5.2]</title>
			<para>解决方案</para>
			<screen>
root@netkiller /var/log/elasticsearch % /usr/share/elasticsearch/bin/elasticsearch-plugin list
analysis-ik
WARNING: plugin [analysis-ik] is incompatible with version [5.6.1]; was designed for version [5.5.2]			
			</screen>
			<screen>
root@netkiller /var/log/elasticsearch % /usr/share/elasticsearch/bin/elasticsearch-plugin remove analysis-ik --purge
-> removing [analysis-ik]...
			</screen>
			<para>手工安装 5.6.0 然后</para>
			<screen>
vim /usr/share/elasticsearch/plugins/analysis-ik/plugin-descriptor.properties 

elasticsearch.version=5.5.2
改为
elasticsearch.version=5.6.1
			</screen>
		</section>
		<section>
			<title>mapper_parsing_exception: failed to parse [ctime]</title>
			<para>date 各位为YYYY-MM-ddTHH:mm:ss，注意中间的字幕T</para>
			<screen>
{"type":"date","format":"YYYY-MM-dd'T'HH:mm:ss.SSSZ"}

curl -XPOST "http://localhost:9200/netkiller/news/" -d'
{
    "content": "Hello World!",
    "CreateDate": "2009-11-15T12:12:12"
}'
			</screen>
		</section>
		<section>
			<title>配置 JAVA_HOME</title>
			<para>编辑 /etc/sysconfig/elasticsearch 配置文件</para>
			<screen>
# Elasticsearch Java path
JAVA_HOME=/srv/java
			</screen>
		</section>
	</section>
</chapter>